import os
import time
import firebase_admin
from firebase_admin import credentials, firestore
from fastapi import FastAPI, HTTPException, WebSocket, WebSocketDisconnect
from pydantic import BaseModel
from typing import List, Optional
from dotenv import load_dotenv
from supabase import create_client, Client
import google.generativeai as genai
from google.api_core import exceptions

# ==========================================
# 1. í™˜ê²½ ì„¤ì • ë° ì´ˆê¸°í™”
# ==========================================
load_dotenv()

# API í‚¤ ë° URL ë¡œë“œ
SUPABASE_URL = "https://wzafalbctqkylhyzlfej.supabase.co"
SUPABASE_KEY = os.getenv("supbase_service_role")
GOOGLE_API_KEY = os.getenv("google_api")
FIREBASE_KEY_PATH = "C:\dxfirebasekey\serviceAccountKey.json"

if not all([SUPABASE_URL, SUPABASE_KEY, GOOGLE_API_KEY]):
    raise ValueError("âŒ í™˜ê²½ë³€ìˆ˜(.env) ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

# 1-1. íŒŒì´ì–´ë² ì´ìŠ¤ ì´ˆê¸°í™”
if not firebase_admin._apps:
    try:
        cred = credentials.Certificate(FIREBASE_KEY_PATH)
        firebase_admin.initialize_app(cred)
        print("ğŸ”¥ íŒŒì´ì–´ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ!")
    except Exception as e:
        print(f"âŒ íŒŒì´ì–´ë² ì´ìŠ¤ í‚¤ íŒŒì¼ ì˜¤ë¥˜: {e}")
        pass

db = firestore.client()

# 1-2. Supabase & Gemini ì´ˆê¸°í™”
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
genai.configure(api_key=GOOGLE_API_KEY)

# ëª¨ë¸ ì„¤ì • (ìµœì‹  ëª¨ë¸ ì ìš©)
EMBEDDING_MODEL = "models/text-embedding-004"
# ì‚¬ìš©ìë‹˜ì´ ì›í•˜ì‹  2.5 ë²„ì „ì´ ì•„ì§ ì •ì‹ ë°°í¬ ì „ì´ë¼ë©´ 2.0-flash-exp ì‚¬ìš© ê¶Œì¥
# ë§Œì•½ 2.5 ì ‘ê·¼ ê¶Œí•œì´ ìˆìœ¼ì‹œë©´ "gemini-2.5-flash"ë¡œ ë°”ê¾¸ì„¸ìš”.
GENERATION_MODEL_ID = "gemini-2.5-flash" 
GENERATION_MODEL = genai.GenerativeModel(GENERATION_MODEL_ID)

print(f"ğŸš€ AI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {GENERATION_MODEL_ID}")


# ==========================================
# 2. í—¬í¼ í•¨ìˆ˜ë“¤
# ==========================================

def save_to_firebase(user_id: str, sender: str, text: str, msg_type: str = "TEXT"):
    try:
        doc_ref = db.collection("chat_rooms").document(f"room_{user_id}").collection("messages")
        doc_ref.add({
            "sender": sender,
            "text": text,
            "message_type": msg_type,
            "timestamp": firestore.SERVER_TIMESTAMP
        })
        print(f"ğŸ’¾ [Firebase] {sender}: {text[:10]}...")
    except Exception as e:
        print(f"âš ï¸ Firebase ì €ì¥ ì‹¤íŒ¨: {e}")

def get_embedding(text: str):
    try:
        result = genai.embed_content(
            model=EMBEDDING_MODEL,
            content=text,
            task_type="retrieval_query"
        )
        return result['embedding']
    except Exception as e:
        print(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
        return None

def optimize_search_query(original_query: str) -> str:
    """ì‚¬ìš©ì ì§ˆë¬¸ì„ ê²€ìƒ‰ìš© í‚¤ì›Œë“œë¡œ ë³€í™˜ (ì¿¼ë¦¬ í™•ì¥)"""
    try:
        prompt = f"""
        ê·œì¹™: ë¬¸ì¥ì´ ì•„ë‹Œ **í‚¤ì›Œë“œ ë‚˜ì—´** í˜•íƒœ. LG ì„¸íƒê¸° ìš©ì–´ ì ê·¹ í™œìš©.
        
        ì‚¬ìš©ì: "{original_query}"
        ë³€í™˜:
        """
        response = GENERATION_MODEL.generate_content(prompt)
        return response.text.strip()
    except Exception as e:
        print(f"âš ï¸ ì¿¼ë¦¬ í™•ì¥ ì‹¤íŒ¨: {e}")
        return original_query


# ==========================================
# 3. FastAPI ì„œë²„ ì„¤ì •
# ==========================================
app = FastAPI()

class ChatRequest(BaseModel):
    user_message: str
    user_id: str

class ChatResponse(BaseModel):
    answer: str
    sources: List[str]

# -------------------------------------------------------
# [API 1] í…ìŠ¤íŠ¸ ì±—ë´‡ (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì ìš©)
# -------------------------------------------------------
@app.post("/chat", response_model=ChatResponse)
async def chat_endpoint(req: ChatRequest):
    print(f"ğŸ“© [ìš”ì²­ ë„ì°©] ID: {req.user_id}, ë‚´ìš©: {req.user_message}")
    
    try:
        # 1. ì‚¬ìš©ì ì§ˆë¬¸ ì €ì¥
        save_to_firebase(req.user_id, "user", req.user_message, "user")  # message_typeì„ senderì™€ ë™ì¼í•˜ê²Œ

        # 2. ì¿¼ë¦¬ í™•ì¥ (í‚¤ì›Œë“œ ê²€ìƒ‰ìš©)
        search_keyword = optimize_search_query(req.user_message)
        print(f"âœ¨ [ì¿¼ë¦¬ í™•ì¥] '{req.user_message}' -> '{search_keyword}'")

        # 3. ì„ë² ë”© ìƒì„± (ë²¡í„° ê²€ìƒ‰ìš©)
        query_vector = get_embedding(search_keyword)
        if not query_vector: raise Exception("ì„ë² ë”© ì‹¤íŒ¨")

        # ğŸ”¥ [í•µì‹¬] í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ RPC í˜¸ì¶œ
        # (Supabaseì— hybrid_search í•¨ìˆ˜ê°€ ë§Œë“¤ì–´ì ¸ ìˆì–´ì•¼ í•¨)
        rpc_response = supabase.rpc("hybrid_search", {
            "query_text": search_keyword,    # í…ìŠ¤íŠ¸ ë§¤ì¹­ìš©
            "query_embedding": query_vector, # ì˜ë¯¸ ê²€ìƒ‰ìš©
            "match_threshold": 0.1,          # ê¸°ì¤€ ì ìˆ˜
            "match_count": 5,                # ê°€ì ¸ì˜¬ ê°œìˆ˜
            "w_vector": 0.9,                 # ë²¡í„° ê°€ì¤‘ì¹˜ (0.0~1.0)
            "w_keyword": 0.1                 # í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ (0.0~1.0)
        }).execute()
        
        search_results = rpc_response.data
        
        if not search_results:
            final_answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ë§¤ë‰´ì–¼ì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê³ ê°ì„¼í„°ì— ë¬¸ì˜í•´ì£¼ì„¸ìš”."
            source_titles = []
        else:
            # 5. í”„ë¡¬í”„íŠ¸ êµ¬ì„± (í•˜ì´ë¸Œë¦¬ë“œ ê²°ê³¼ ì‚¬ìš©)
            context_list = []
            for item in search_results:
                # hybrid_search í•¨ìˆ˜ëŠ” 'content_text'ë¡œ ë¦¬í„´í•¨
                text = item.get('content_text') or item.get('content') or ""
                title = item.get('section_title') or "ì •ë³´"
                context_list.append(f"- {text} (ì¶œì²˜: {title})")
            
            context_text = "\n\n".join(context_list)
            source_titles = list(set([item.get('section_title', 'ì œëª©ì—†ìŒ') for item in search_results]))

            prompt = f"""
            ë‹¹ì‹ ì€ LGì „ì ê°€ì „ì œí’ˆ ì „ë¬¸ ìƒë‹´ì› 'ThinQ ë´‡'ì…ë‹ˆë‹¤.
            ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì•„ë˜ ì œê³µëœ [ë§¤ë‰´ì–¼ ë°ì´í„°]ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.
            ì„¸íƒë°©ë²•ì— ëŒ€í•´ ë¬¼ì—ˆëŠ”ë° ë©”ë‰´ì–¼ì— ì—†ë‹¤ë©´ ë‹¤ë¥¸ íŠ¹ì • ì„¸íƒê¸°ì˜ ê¸°ëŠ¥ì€ ë§í•˜ì§€ ë§ê³  íŠ¹ì • ì„¸íƒê¸°ê°€ ì—†ì–´ë„ ëˆ„êµ¬ë‚˜ ì ìš©ê°€ëŠ¥í•œ ë°©ë²•ì„ ë„ˆê°€ ì•Œê³  ìˆëŠ” ìµœëŒ€í•œ ì •í™•í•œ ì§€ì‹ìœ¼ë¡œ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì¤˜
            
            [ì§€ì¹¨]
            1. í‘œ ë‚´ìš©ì€ ë¬¸ì¥ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ í’€ì–´ì„œ ì„¤ëª…í•˜ì„¸ìš”.
            2. ë‹µë³€ ëì—ëŠ” ì°¸ê³ í•œ í˜ì´ì§€ ë²ˆí˜¸ë‚˜ ì„¹ì…˜ì„ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.
            3. ì‚¬ìš©ìê°€ 'í†µëŒì´', 'ë“œëŸ¼' ë“± êµ¬ì–´ì²´ë¥¼ ì¨ë„, ë§¤ë‰´ì–¼ì˜ í•´ë‹¹ ì œí’ˆêµ° ë‚´ìš©ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
            4. ì§ˆë¬¸ì— 'ëµí'ê°€ ìˆë‹¤ë©´ ë‹µë³€í•  ë•Œ 'LG ThinQ'ë¡œ ë°”ê¿”ì„œ ë§í•´ì£¼ì„¸ìš”.
            
            [ë§¤ë‰´ì–¼ ë°ì´í„°]:
            {context_text}
            
            [ì‚¬ìš©ì ì§ˆë¬¸]: {req.user_message}
            (ì°¸ê³ : '{search_keyword}' ê´€ë ¨ ë‚´ìš©ì„ ê²€ìƒ‰í–ˆìŠµë‹ˆë‹¤.)
            
            [ë‹µë³€]:
            """
            
            # 6. ë‹µë³€ ìƒì„±
            gen_resp = GENERATION_MODEL.generate_content(prompt)
            final_answer = gen_resp.text

        # 7. ë‹µë³€ ì €ì¥
        save_to_firebase(req.user_id, "ai", final_answer, "ai")  # message_typeì„ senderì™€ ë™ì¼í•˜ê²Œ
        print(f"âœ… [ë‹µë³€ ì™„ë£Œ] {final_answer[:30]}...")

        return ChatResponse(
            answer=final_answer,
            sources=source_titles
        )

    except Exception as e:
        print(f"âŒ ì„œë²„ ì—ëŸ¬: {e}")
        return ChatResponse(
            answer=f"ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ({str(e)})",
            sources=[]
        )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)